{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åœºæ™¯ä¸€ï¼šè§’è‰²æ‰®æ¼”çš„å†™ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 8359bebea988... 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (7.4/7.4 GB, 16 TB/s)        \n",
      "pulling 65c6ec5c6ff0... 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (45/45 B, 1.8 MB/s)        \n",
      "pulling dd36891f03a0... 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (31/31 B, 1.3 MB/s)        \n",
      "pulling f94f529485e6... 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| (382/382 B, 17 MB/s)        \n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama2-chinese:13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ‘‹ç¥å¤§å®¶å¥½ï¼æˆ‘ä»Šå¤©ç‰¹åœ°ä¸ºå¤§å®¶æ¨èäº†ä¸€æœ¬æ–°ä¹¦ã€ŠLangChainå®æˆ˜ã€‹ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å¼€å§‹æ¥å­¦ä¹  LangChain å§ï¼ğŸ“šğŸ’»è¿™æœ¬ä¹¦æ˜¯ç”±ä¸€äº›ä¸“ä¸šçš„æŠ€æœ¯äººå‘˜ç¼–å†™çš„ï¼Œå†…å®¹ååˆ†é€å½»ã€å®ç”¨ã€‚ğŸ¤å¦‚æœå¤§å®¶æƒ³è¦æé«˜è‡ªå·±çš„ç¼–ç¨‹æ°´å¹³ï¼Œæˆ–è€…äº†è§£æ›´å¤šçš„å¼€å‘æ¡†æ¶ï¼Œè¿™æœ¬ä¹¦ä¸€å®šä¼šå¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚ğŸ‘èµï¼\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# è®¾å®šç³»ç»Ÿä¸Šä¸‹æ–‡ï¼Œæ„å»ºæç¤ºè¯\n",
    "template = \"\"\"è¯·æ‰®æ¼”ä¸€ä½èµ„æ·±çš„æŠ€æœ¯åšä¸»ï¼Œæ‚¨å°†è´Ÿè´£ä¸ºç”¨æˆ·ç”Ÿæˆé€‚åˆåœ¨å¾®åšå‘é€çš„ä¸­æ–‡å¸–æ–‡ã€‚\n",
    "è¯·æŠŠç”¨æˆ·è¾“å…¥çš„å†…å®¹æ‰©å±•æˆ 140 å­—å·¦å³çš„æ–‡å­—ï¼Œå¹¶åŠ ä¸Šé€‚å½“çš„ emoji ä½¿å†…å®¹å¼•äººå…¥èƒœå¹¶ä¸“ä¸šã€‚\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "\n",
    "# é€šè¿‡ Ollama åŠ è½½ Llama 2 13B å¯¹è¯è¡¥å…¨æ¨¡å‹\n",
    "model = ChatOllama(model=\"llama2-chinese:13b\")\n",
    "\n",
    "# é€šè¿‡ LCEL æ„å»ºè°ƒç”¨é“¾å¹¶æ‰§è¡Œå¾—åˆ°æ–‡æœ¬è¾“å‡º\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({ \"input\": \"ç»™å¤§å®¶æ¨èä¸€æœ¬æ–°ä¹¦ã€ŠLangChainå®æˆ˜ã€‹ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å¼€å§‹æ¥å­¦ä¹  LangChain å§ï¼\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\nå°ç™½å…”åœ¨èŠ±è‰é—´è·ƒæ­¥è€Œæ¥ã€‚çœ‹åˆ°äº†ä¸€é¢—ç¾å‘³çš„è‰æ ¹ï¼Œä¾¿ç”¨é•¿é•¿çš„é•¿ç€æƒ³åƒä¸‹å»ã€‚å´å‘ç°è¿™åªä¸è¿‡æ˜¯ä¸€ä¸ªä¼¼äººåƒçš„ç©ç‰©ã€‚å°ç™½å…”å¼€å§‹ç¬‘èµ·æ¥ï¼Œæ„Ÿåˆ°ååˆ†æƒŠå–œã€‚ä»–æ‡‚å¾—å¦‚ä½•äº«å—ç”Ÿæ´»ä¸­çš„ç¾å¥½äº‹ç‰©ã€‚\\n')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"è¯·ç¼–å†™ä¸€ç¯‡å…³äº{topic}çš„ä¸­æ–‡å°æ•…äº‹ï¼Œä¸è¶…è¿‡100å­—\")\n",
    "model = ChatOllama(model=\"llama2-chinese:13b\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"topic\": \"å°ç™½å…”\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about rabbit.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"rabbit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: tall\n",
      "Output: short\n",
      "\n",
      "Input: energetic\n",
      "Output: lethargic\n",
      "\n",
      "Input: sunny\n",
      "Output: gloomy\n",
      "\n",
      "Input: windy\n",
      "Output: calm\n",
      "\n",
      "Input: big\n",
      "Output:\n",
      "\n",
      "\n",
      "Give the antonym of every input\n",
      "\n",
      "Input: happy\n",
      "Output: sad\n",
      "\n",
      "Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\n",
      "Output:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "# åˆ›å»ºä¸€äº›åä¹‰è¯è¾“å…¥è¾“å‡ºçš„ç¤ºä¾‹å†…å®¹\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    # è®¾å®šæœŸæœ›çš„ç¤ºä¾‹æ–‡æœ¬é•¿åº¦\n",
    "    max_length=25\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    # è®¾ç½®ç¤ºä¾‹ä»¥å¤–éƒ¨åˆ†çš„å‰ç½®æ–‡æœ¬\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # è®¾ç½®ç¤ºä¾‹ä»¥å¤–éƒ¨åˆ†çš„åç½®æ–‡æœ¬\n",
    "    suffix=\"Input: {adjective}\\nOutput:\\n\\n\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")\n",
    "\n",
    "# å½“ç”¨æˆ·è¾“å…¥çš„å†…å®¹æ¯”è¾ƒå°‘æ—¶ï¼Œæ‰€æœ‰ç¤ºä¾‹éƒ½è¶³å¤Ÿè¢«ä½¿ç”¨\n",
    "print(dynamic_prompt.format(adjective=\"big\"))\n",
    "\n",
    "# å½“ç”¨æˆ·è¾“å…¥çš„å†…å®¹è¶³å¤Ÿé•¿æ—¶ï¼Œåªæœ‰å°‘é‡ç¤ºä¾‹ä¼šè¢«å¼•ç”¨\n",
    "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='è¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š\\néšæœºç”Ÿæˆä¸€ä½çŸ¥åçš„ä½œå®¶åŠå…¶ä»£è¡¨ä½œå“\\n\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of an author\", \"type\": \"string\"}, \"book_names\": {\"title\": \"Book Names\", \"description\": \"list of names of book they wrote\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"name\", \"book_names\"]}\\n```\\nå¦‚æœè¾“å‡ºæ˜¯ä»£ç å—ï¼Œè¯·ä¸è¦åŒ…å«é¦–å°¾çš„```ç¬¦å·'\n",
      "{\n",
      "    \"name\": \"J.K. Rowling\",\n",
      "    \"book_names\": [\n",
      "        \"Harry Potter and the Philosopher's Stone\", \n",
      "        \"Harry Potter and the Chamber of Secrets\", \n",
      "        \"Harry Potter and the Prisoner of Azkaban\", \n",
      "        \"Harry Potter and the Goblet of Fire\", \n",
      "        \"Harry Potter and the Order of the Phoenix\", \n",
      "        \"Harry Potter and the Half-Blood Prince\", \n",
      "        \"Harry Potter and the Deathly Hallows\"\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Actor(name='J.K. Rowling', book_names=[\"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Order of the Phoenix', 'Harry Potter and the Half-Blood Prince', 'Harry Potter and the Deathly Hallows'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an author\")\n",
    "    book_names: List[str] = Field(description=\"list of names of book they wrote\")\n",
    "\n",
    "\n",
    "actor_query = \"éšæœºç”Ÿæˆä¸€ä½çŸ¥åçš„ä½œå®¶åŠå…¶ä»£è¡¨ä½œå“\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Actor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"è¯·å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š\\n{query}\\n\\n{format_instructions}\\nå¦‚æœè¾“å‡ºæ˜¯ä»£ç å—ï¼Œè¯·ä¸è¦åŒ…å«é¦–å°¾çš„```ç¬¦å·\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "input = prompt.format_prompt(query=actor_query)\n",
    "print(input)\n",
    "\n",
    "model = Ollama(model=\"llama2-chinese:13b\")\n",
    "output = model(input.to_string())\n",
    "\n",
    "print(output)\n",
    "parser.parse(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
